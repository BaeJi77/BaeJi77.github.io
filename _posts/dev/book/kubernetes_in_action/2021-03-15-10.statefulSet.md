---
title: "[Kubernetes in action] chapter 10. statefulset"

categories:
  - dev
  - book
tags:
  - dev
  - book
  - kubernetes in action
---

# 서론
- 클러스터된 스테이트풀 애플리케이션 배포
- 파드 레플리카 인스턴스에 별도의 스토리지 제공
- 파드 레플리카에 안정적인 이름과 호스트 이름 보장
- 예측 가능한 순서대로 파드 레플리카의 시작과 중지
- DNS 서비스 레코드를 통한 피어 디스커버리

# 스테이트풀 파드 복제
레플리카셋은 이름과 ip 주소를 제외하고 모두 똑같은 파드를 만듬. 

그리고 모두 똑같은 볼륨을 사용함. = 다른 퍼시스턴트볼륨클레임은 사용 불가

## 레플리카에 개별 스토리지 붙이기
- 수동으로 파드 생성
  - 나중에 너무 귀찮음. 죽으면 계속 살려야되는데 말이 안됨.


- 파드 인스턴스별로 하나의 레플리카셋 사용
  - 단일 레플리카를 사용하는 것에 비해 너무 불편함.
  - 하나 배포한다고 해도 각 레플리카에 대해서 배포를 직접 해야됨
  - 스케일도 귀찮음
![eachpod](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig02.jpg)

- 동일 볼륨을 여러 개 디렉토리로 분할 사용
  - 하나의 퍼시스턴스 볼륨 + 각 파트들이 다른 디렉토리 공간 지정해서 사용
  - 인스턴스가 생성되는 시점에 사용하지 않는 디렉토리를 선택할 수는 있음.
  - 하지만 잘 동작하는 것에 대한 보장이 안되고 볼륨 병목 현상 발생

![eachPodSameVolume](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig03_alt.jpg)

## 각 파드에 안정적인 아이덴티티 제공
새로운 파드를 교체해야되는 경우 레플리카셋같은 새로운 호스트 네임과 Ip를 제공한다.

이전 인스턴스 데이터를 가지고 시작할 때 네트워크 아이덴티티로 인한 문제가 발생한다. 

넘버링 (index)를 이름에 붙인다.

![comparingReplicaAndStateful](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig05_alt.jpg)

### 각 파드 인스턴스별 전용 서비스 사용
서비스같은 경우 변경되지 않는 ip를 제공한다고 생각하기 때문에 파드별로 서비스를 가지고 있으면 새로운 파드가 만들어 진다고해도 문제가 되지 않지 않을까?

할수는 있겠지만 각 파드가 어떤 서비스에 매핑되는지 알기 어렵기 때문에 해당 파드에 접근하기가 어렵다.

### 거버닝 서비스
동일한 이름말고도 호스트를 통해서 어떤 접근을 해야될때가 있음.

그 뿐만 아니라 스테이트리스와 다르게 스테이트풀 파드들은 각자 상태가 다르기 때문에 특별히 인식할 필요가 있다.

그래서 거버닝 헤드리스 서비스를 생성해서 호스트에 대한 네트워크 아이덴티티를 제공

```
namespace: default
service name: foo
pod name: a-0

FQDN: a-0.foo.default.svc.cluster.local
```

또한 SRV 레코드를 조회해 모든 스테이트풀셋의 파드이름을 가져올 수 있음.

# 스테이트풀셋

## 스테이트풀셋과 레플리카셋 비교

- 레플리카셋
다른 파드들과 거의 대부분 동일. 이전 파드와 새로운 파드가 식별만 다르지 동작은 그래도.

대부분 스테이트리스로 언제든지 완전히 새로운 파드로 교체가능.

- 스테이트풀셋
스테이트풀 파드는 새로운 파드 인스턴스는 이전 파드와 동일한 이름, 네트워크 아이덴티티, 상태를 똑같이 있어야된다. 

replica의 수를 줄이고 늘리는대도 규칙이 있다. 줄였다가 다시 늘리는데 이전 파드와 동일한 정보

![comparing_2](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig06_alt.jpg)

### 스테이트풀셋 스케일링
만약 2개의 레플리카 파드가 있다가 과정했을 때 새로운 파드에 이름에는 다음 인덱스인 2가 붙여서 생성될 것이다. 

ex) a-0, a-1, a-2 (New)

또한 삭제될 때는 그 순서에 반대대로 삭제된다.

ex) a-0, a-1, a-2 (will be removed), a-3 (be removed)

![scailing](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig07_alt.jpg)

### 각 스테이트풀 인트선스에 안정적인 스토리지 제공
새로 만들어진 파드에 대해서 이전 스토리지와 같은 스트리지가 할당되도록 해야된다.

스토리지는 영구적이어야ㅚ지만 파드와는 분리되어야한다.

6장에서 배운 것처럼 퍼시스턴트볼륨클레임을 이용해서 볼륨과는 1:1 매핑이 진행되고 파드와 퍼시스턴스볼륨 클레임을 1:1 매핑해서 각자 스토리지를 가지도록 한다.

``` text
pod 1 : 1 pvc 
pvc 1 : 1 pv

pod -> pvc -> pv
```

이렇게 하기 위해서 볼륨 클레임 템플릿을 이용한다. (팟을 선언적으로 만들도록 했던 템플릿과 비슷)

![볼륨클레임템플릿](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig08_alt.jpg)

이 가정에는 관리자가 프로비저닝한다거나 혹은 동적 프로비저닝이 지원한다고 가정

> 주의: 스케일 다운을 하고 남은 볼륨이 중요한 데이터라면 직접 관리하는 것이 중요하다. (퍼시스턴트볼륨클레임을 직접 삭제)
> 다시 스케일 업을 하게 되는 경우 데이터가 손상되거나, 전략에 의해서 데이터가 삭제될 수도 있다.

![새로운파드볼륨](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig09_alt.jpg)

# 스테이트풀셋 사용

## format
``` yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: hoonSts
spec:
  replicas: 3
  template:
    metadata:
      name: hoonPod
      labels:
        app: hoonApp
    spec:
      containers:
      - image: github/hoonImage:v1
        name: hoonContainer
volumeCliamTemplates:
- metadata:
    name: data
  spec:
    resources:
      request:
      storage: 1Mi
    accessModes:
    - ReadWriteOnce
```
deployment와 다르게 sts는 파드가 모두 완벽하게 떴을 경우에는 다음 파드를 만든다.

두개를 동시에 만들다가 레이스 컨디션을 발생할 가능성이 있기 때문이다.


- 실행 이후 모습
``` yaml
$ kubectl get po kubia-0 -o yaml

---
apiVersion: v1
kind: Pod
metadata:
  ...
spec:
  containers:
  - image: luksa/kubia-pet
    ...
    volumeMounts:
    - mountPath: /var/data
      name: data
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-r2m41
      readOnly: true
  ...
  volumes:
  - name: data
    persistentVolumeClaim:
      claimName: data-kubia-0
  - name: default-token-r2m41
    secret:
      secretName: default-token-r2m41
```

### 실제 pvc 모습
``` bash
$ kubectl get pvc
NAME           STATUS    VOLUME    CAPACITY   ACCESSMODES   AGE
data-kubia-0   Bound     pv-c      0                        37s
data-kubia-1   Bound     pv-a      0                        37s
```

# 스테이트풀셋의 피어 디스커버리
## 헤드레스트 서비스
``` yaml
apiVersion: v1
kind: Service
metadata:
  name: kubia
spec:
  clusterIP: None
  selector:
    app: kubia
  ports:
  - name: http
    port: 80
```
거버닝 서비스를 위해서 헤드레스트 서비스가 필요함.

### SRV 레코드
특정 서비스를 제공하는 서버의 호스트 이름과 포트를 가리키는데 사용.

쿠버네티스는 헤드리스 서비스를 뒷밤침하는 파드의 호스트 이름을 가리키도록 SRV 레코드를 생성.

``` bash
dig SRV kubia.default.svc.cluster.local

...
;; ANSWER SECTION:
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-0.kubia.default.svc.cluster.local.
k.d.s.c.l. 30 IN  SRV     10 33 0 kubia-1.kubia.default.svc.cluster.local.

;; ADDITIONAL SECTION:
kubia-0.kubia.default.svc.cluster.local. 30 IN A 172.17.0.4
kubia-1.kubia.default.svc.cluster.local. 30 IN A 172.17.0.6
...
```

위의 모습처럼 파드가 스테이트풀셋의 모든 파드 목록을 가져오려면 SRV DNS 룩업을 수행하기만 하면 된다.

## DNS를 통한 피어 디스커버리
쿠버네티스 서버는 모든 파드들에 대한 정보를 가지고 있다. 

하지만 클라이언트는 모른다. 알기 위해서 서버에 요청을 해야되는데 너무 번거롭고 시간도 오래 걸림.

그런 상황에서 스테이트풀셋과 SRV 레코드를 사용하면 편하게 해결 가능하다.

![dnsLookUp](https://drek4537l1klr.cloudfront.net/luksa/Figures/10fig12_alt.jpg)

## 클러스터된 데이터 저장소 사용
전략에 따라 다를 수 있겠지만 우리는 스케일 다운해서 새로운 올린 파드는 이전 파드에 연결되어진 스토리지가 있다.

그러기 때문에 이전에 있던 파일정도 스토리지에 저장된 결과는 달라지지 않는다. 그리고 바로 반환 가능하다.


# 스테이트풀셋 노드 실패를 처리하는 과정

책을 참조...


# 궁금증
왜 클러스턴된 애플리케이션에서는 피어 디스커버리(동일 클러스터에 있는 다른 멤버를 찾는 기능)

# ref
책: 쿠버네티스 인 액션

이미지 출처: https://livebook.manning.com/book/kubernetes-in-action/chapter-10/55

